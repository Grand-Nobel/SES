# integration-layer/terraform/main.tf

terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.0.0"
    }
    # Add other providers if needed, e.g., helm, vault
    # helm = {
    #   source  = "hashicorp/helm"
    #   version = ">= 2.0.0"
    # }
  }
}

# Configure the Kubernetes provider
# Assumes kubectl is configured to point to the correct cluster
provider "kubernetes" {
  # config_path = "~/.kube/config" # Or other path to kubeconfig if not default
}

# Variable for Supabase API key
variable "supabase_key" {
  description = "Supabase API key (used by the application, not directly by this TF config for provisioning Supabase itself)"
  type        = string
  sensitive   = true
  # default     = "your_supabase_key_here" # Avoid setting a default for sensitive variables
}

# Variable for Vault address
variable "vault_addr" {
  description = "Address of the HashiCorp Vault server"
  type        = string
  default     = "https://vault.ses.com" # Default as per outline
}

# Local module for SES Orchestration specific resources (if any beyond Helm)
# This module block is defined once.
module "ses_orchestration_custom" { # Renamed to avoid conflict if a Helm release is also named this
  source = "./modules/ses-orchestration" # Local module path

  # Pass variables to the local module if it defines any
  # Example:
  # supabase_url = "https://supabase.ses.com" # This would be defined in the module's variables.tf
  # supabase_key = var.supabase_key
  # kafka_brokers = ["broker1.ses.com:9092", "broker2.ses.com:9092"]
  # redis_host    = "redis.ses.com"
  # vault_addr    = var.vault_addr
}

# The following kubernetes_manifest resources assume that
# orchestrator-rollout.yaml and agent-runner-rollout.yaml exist.
# These would typically be generated by `helm template` or be part of the Helm chart's templates.
# If these are ArgoCD Rollout custom resources, ensure the CRDs are present in the cluster.

# Placeholder for orchestrator-rollout.yaml - this file needs to exist
# or be generated by another process (e.g., helm template > file)
# For now, we'll assume it might be created manually or by a CI/CD step.
# To make this runnable, these files would need content.
# Example:
# resource "local_file" "orchestrator_rollout_yaml" {
#   content  = "# Placeholder for orchestrator-rollout.yaml content"
#   filename = "${path.module}/../helm/ses-orchestration/templates/orchestrator-rollout.yaml"
# }
# resource "local_file" "agent_runner_rollout_yaml" {
#   content  = "# Placeholder for agent-runner-rollout.yaml content"
#   filename = "${path.module}/../helm/ses-orchestration/templates/agent-runner-rollout.yaml"
# }

# resource "kubernetes_manifest" "orchestrator_rollout" {
#   # Ensure the file path is correct and the file exists.
#   # This assumes the Helm chart templates are structured to output these specific files,
#   # or they are manually created/managed.
#   # The path.module here refers to the directory of this main.tf file.
#   manifest = yamldecode(file("${path.module}/../helm/ses-orchestration/templates/orchestrator-rollout.yaml"))
#   # depends_on = [local_file.orchestrator_rollout_yaml] # If using local_file to create placeholders
# }

# resource "kubernetes_manifest" "agent_runner_rollout" {
#   manifest = yamldecode(file("${path.module}/../helm/ses-orchestration/templates/agent-runner-rollout.yaml"))
#   # depends_on = [local_file.agent_runner_rollout_yaml] # If using local_file to create placeholders
# }

# Note: The original file had a duplicate module "ses_orchestration" block.
# It's more common to manage Helm chart deployments using the Helm provider in Terraform.
# Example using Helm provider (alternative to kubernetes_manifest for Helm charts):
#
# provider "helm" {
#   kubernetes {
#     config_path = "~/.kube/config" # Or your kubeconfig path
#   }
# }
#
# resource "helm_release" "ses_orchestration_chart" {
#   name       = "ses-orchestration"
#   repository = "../helm" # Path to the directory containing the ses-orchestration chart
#   chart      = "ses-orchestration"
#   namespace  = "default" # Or your target namespace
#
#   values = [
#     "${file("${path.module}/../helm/ses-orchestration/values.yaml")}"
#   ]
#
#   # If you need to pass specific values:
#   # set {
#   #   name  = "orchestrator.replicas"
#   #   value = "3"
#   # }
#   # set {
#   #   name  = "supabase.externalName" # Example from values.yaml
#   #   value = "custom.supabase.url.com"
#   # }
# }

# For the provided structure, the kubernetes_manifest resources are kept,
# but it implies that orchestrator-rollout.yaml and agent-runner-rollout.yaml
# are actual files in that location, which is unusual for Helm templates directly.
# If these are Argo Rollouts, their definitions would be in those YAML files.
# The Helm chart itself would typically deploy these if they were part of its templates.

# The prompt implies these files exist or will be created.
# If they are NOT part of the Helm chart's standard output (e.g. if they are Argo Rollouts)
# then their content needs to be defined.
# For now, I will keep the kubernetes_manifest resources as requested,
# assuming the YAML files they point to will be provided or exist.
# If these files are missing, terraform apply will fail.

resource "kubernetes_manifest" "orchestrator_rollout" {
  # This assumes a file named orchestrator-rollout.yaml exists in the specified Helm templates directory.
  # This is not a standard Helm practice; templates are usually not directly applied like this via Terraform.
  # Typically, you'd use the helm_release resource or `helm template ... | kubectl apply -f -`.
  # For the purpose of fulfilling the request, I'm including it as written.
  # The file content for orchestrator-rollout.yaml would need to be a valid Kubernetes manifest (e.g., an Argo Rollout).
  manifest = yamldecode(file("${path.module}/../helm/ses-orchestration/templates/orchestrator-rollout.yaml"))
}

resource "kubernetes_manifest" "agent_runner_rollout" {
  manifest = yamldecode(file("${path.module}/../helm/ses-orchestration/templates/agent-runner-rollout.yaml"))
}

# To make the above resources work, placeholder files for the rollouts will be created.
# These should be replaced with actual Argo Rollout definitions.